{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from extract_features import mySigLipModel\n",
    "import time\n",
    "import numpy as np\n",
    "import requests\n",
    "from PIL import Image\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = './indexed_100k-200k/siglip_image_urls-120k-125k.json'\n",
    "index_path = './indexed_100k-200k/siglip-image-index-120k-125k.bin'\n",
    "dataset_caption_path = './dataset/SBU_captioned_photo_dataset_captions.txt'\n",
    "dataset_url_path = './dataset/SBU_captioned_photo_dataset_urls.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in the embedding extractor\n",
    "extractor = mySigLipModel()\n",
    "#load in indexer\n",
    "index = faiss.read_index(index_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_size = 50\n",
    "k_list = [3, 5, 10, 20, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = 120000\n",
    "end_index = 125000\n",
    "with open(dataset_caption_path, 'r') as f:\n",
    "    captions = f.readlines()[start_index:end_index]\n",
    "with open(dataset_url_path, 'r') as f:\n",
    "    urls = f.readlines()[start_index:end_index]\n",
    "with open(image_path, 'r') as f:\n",
    "    train_image_urls = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select (testing_size * 1.5) images from the dataset in random\n",
    "selecting_size = int(testing_size * 1.5)  # 20% more for the case that some images are not valid\n",
    "np.random.seed(0)\n",
    "selected_indices = np.random.choice(len(captions), selecting_size, replace=False)\n",
    "selected_captions = [captions[i] for i in selected_indices]\n",
    "selected_urls = [urls[i] for i in selected_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the Recall of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@3: 0.14\n",
      "Recall@5: 0.22\n",
      "Recall@10: 0.34\n",
      "Recall@20: 0.46\n",
      "Recall@50: 0.5\n"
     ]
    }
   ],
   "source": [
    "# compute the recall@k\n",
    "for k in k_list:\n",
    "    recall = 0\n",
    "    count = 0\n",
    "    for i in range(selecting_size):\n",
    "        url = selected_urls[i]\n",
    "        # use regex to extract the image id in the pattern of 'http://static.flickr.com/[image_id]/XXX.jpg'\n",
    "        image_id = int(re.search(r'http://static.flickr.com/(\\d+)/', url).group(1))\n",
    "        # print('image_id: {}'.format(image_id))\n",
    "        caption = selected_captions[i]\n",
    "\n",
    "        # check if the image is valid\n",
    "        try:\n",
    "            frame = Image.open(requests.get(url, stream=True).raw)\n",
    "            count += 1\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        # get the embedding of the caption\n",
    "        query_embedding = extractor.get_text_embedding(caption)\n",
    "\n",
    "        # search the k nearest neighbors\n",
    "        D, I = index.search(query_embedding, k)\n",
    "        # check if the caption of the image is in the k nearest neighbors\n",
    "        result_urls = [train_image_urls[j].strip() for j in I[0]]\n",
    "        result_ids = [int(re.search(r'http://static.flickr.com/(\\d+)/', result_url).group(1)) for result_url in result_urls]\n",
    "        \n",
    "        # print('Query: {}'.format(caption))\n",
    "        # print('Query url: {}'.format(url))\n",
    "        # print('Results:')\n",
    "        # for j in range(k):\n",
    "        #     print('id: {}, url: {}'.format(result_ids[j], result_urls[j]))\n",
    "\n",
    "        if image_id in result_ids:\n",
    "            recall += 1\n",
    "\n",
    "        if count >= testing_size:\n",
    "            break\n",
    "\n",
    "    print('Recall@{}: {}'.format(k, recall / testing_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the search time of L2 and HNSW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 50\n",
    "l2_indexing_time = 0\n",
    "hnsw_indexing_time = 0\n",
    "\n",
    "l2_indexing_path = './l2_index/0-100k.bin'\n",
    "hnsw_indexing_path = './hnsw_index/0-100k.bin'\n",
    "dataset_caption_path = './dataset/SBU_captioned_photo_dataset_captions.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = 0\n",
    "end_index = 100000\n",
    "\n",
    "l2_index = faiss.read_index(l2_indexing_path)\n",
    "hnsw_index = faiss.read_index(hnsw_indexing_path)\n",
    "with open(dataset_caption_path, 'r') as f:\n",
    "    captions = f.readlines()[start_index:end_index]\n",
    "with open(dataset_url_path, 'r') as f:\n",
    "    urls = f.readlines()[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_size = 100\n",
    "np.random.seed(0)\n",
    "selected_indices = np.random.choice(len(captions), testing_size, replace=False)\n",
    "selected_captions = [captions[i] for i in selected_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flat-l2 indexing time: 7.276071071624756\n",
      "HNSW indexing time: 6.214844465255737\n"
     ]
    }
   ],
   "source": [
    "# l2\n",
    "start = time.time()\n",
    "for caption in selected_captions:\n",
    "    query_embedding = extractor.get_text_embedding(caption)\n",
    "    D, I = l2_index.search(query_embedding, k)\n",
    "end = time.time()\n",
    "l2_indexing_time = end - start\n",
    "print('Flat-l2 indexing time: {}'.format(l2_indexing_time))\n",
    "\n",
    "# hnsw\n",
    "start = time.time()\n",
    "for caption in selected_captions:\n",
    "    query_embedding = extractor.get_text_embedding(caption)\n",
    "    D, I = hnsw_index.search(query_embedding, k)\n",
    "end = time.time()\n",
    "hnsw_indexing_time = end - start\n",
    "print('HNSW indexing time: {}'.format(hnsw_indexing_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the recall@50 of L2 and HNSW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of image link to filter out\n",
    "filter_out = [\n",
    "    \"https://static.flickr.com/1118/1194875137_25885364be.jpg\",\n",
    "   \"https://static.flickr.com/2636/3935060630_7dcf980757.jpg\",\n",
    "    \"https://static.flickr.com/216/523409934_f0d9aa5a96.jpg\",\n",
    "   \"https://static.flickr.com/3147/3056994559_e9f6d21555.jpg\",\n",
    "    \"https://static.flickr.com/25/65770723_9f1921f2bb.jpg\",\n",
    "    \"https://static.flickr.com/2440/3634933857_a7c572d56f.jpg\",\n",
    "    \"https://static.flickr.com/5178/5489673855_81d8916bef.jpg\",\n",
    "    \"https://static.flickr.com/4020/4671844388_2b294f83d4.jpg\",\n",
    "   \"https://static.flickr.com/2490/3816574357_3d657db043.jpg\",\n",
    "   \"https://static.flickr.com/3596/3405303362_4b1b12d135.jpg\",\n",
    "   \"https://static.flickr.com/3320/3297671530_dfce477ca7.jpg\",\n",
    "   \"https://static.flickr.com/1249/1446311820_59accba36c.jpg\",\n",
    "   \"https://static.flickr.com/5178/5489673855_81d8916bef.jpg\",\n",
    "   \"https://static.flickr.com/114/279430835_08bcf39b98.jpg\",\n",
    "   \"https://static.flickr.com/5047/5216626543_5a340330b7.jpg\",\n",
    "   \"https://static.flickr.com/2684/4520382314_eb82d277cc.jpg\",\n",
    "   \"https://static.flickr.com/1023/593085881_d937b91335.jpg\",\n",
    "   \"https://static.flickr.com/4020/4671844388_2b294f83d4.jpg\",\n",
    "   \"https://static.flickr.com/2490/3816574357_3d657db043.jpg\",\n",
    "   \"https://static.flickr.com/1249/1446311820_59accba36c.jpg\",\n",
    "   \"https://static.flickr.com/4020/5078251694_7b3a9c03c4.jpg\",\n",
    "   \"https://static.flickr.com/2684/4520382314_eb82d277cc.jpg\",\n",
    "   \"https://static.flickr.com/4113/5041050280_b445ff4505.jpg\",\n",
    "   \"http://static.flickr.com/1225/1347132376_85bee547a0.jpg\",\n",
    "\n",
    "\n",
    "]\n",
    "\n",
    "filter_out1 = [\n",
    "    \"https://static.flickr.com/4051/4227721866_c52e04a94c.jpg\",\n",
    "    \"https://static.flickr.com/3198/3054356126_50e05efecb.jpg\",\n",
    "    \"https://static.flickr.com/216/523409934_f0d9aa5a96.jpg\",\n",
    "    \"https://static.flickr.com/3069/2632441642_b38b5fbf72.jpg\",\n",
    "    \"https://static.flickr.com/2677/4332578964_68faa48446.jpg\",\n",
    "    \"https://static.flickr.com/4064/4317883247_33dabacf5b.jpg\",\n",
    "]\n",
    "filtered_urls = [url for url in filter_out + filter_out1]\n",
    "\n",
    "\n",
    "# Function to extract image ID from URL\n",
    "def extract_image_id(url):\n",
    "    try:\n",
    "        # Modified regex to match both http and https\n",
    "        image_id = int(re.search(r'https?://static.flickr.com/(\\d+)/', url).group(1))\n",
    "        return image_id\n",
    "    except AttributeError:\n",
    "        # This will handle the case where the regex search finds no match\n",
    "        print(f\"No image ID found in URL: {url}\")\n",
    "        return None\n",
    "\n",
    "# List to hold the image IDs\n",
    "filter_ids = []\n",
    "\n",
    "# Iterate over each URL and extract the image ID\n",
    "for url in filtered_urls:\n",
    "    id = extract_image_id(url)\n",
    "    if id is not None:\n",
    "        filter_ids.append(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 50\n",
    "l2_indexing_time = 0\n",
    "hnsw_indexing_time = 0\n",
    "\n",
    "l2_indexing_path = './l2_index/0-100k.bin'\n",
    "hnsw_indexing_path = './hnsw_index/0-100k.bin'\n",
    "dataset_caption_path = './dataset/SBU_captioned_photo_dataset_captions.txt'\n",
    "\n",
    "l2_image_path = './l2_index/0-100k.json'\n",
    "hnsw_image_path = './hnsw_index/0-100k.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = 0\n",
    "end_index = 100000\n",
    "\n",
    "l2_index = faiss.read_index(l2_indexing_path)\n",
    "hnsw_index = faiss.read_index(hnsw_indexing_path)\n",
    "with open(dataset_caption_path, 'r') as f:\n",
    "    captions = f.readlines()[start_index:end_index]\n",
    "with open(dataset_url_path, 'r') as f:\n",
    "    urls = f.readlines()[start_index:end_index]\n",
    "with open(l2_image_path, 'r') as f:\n",
    "    l2_train_image_urls = json.load(f)\n",
    "with open(hnsw_image_path, 'r') as f:\n",
    "    hnsw_train_image_urls = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_size = 100\n",
    "selecting_size = int(testing_size * 1.5)  # 20% more for the case that some images are not valid\n",
    "np.random.seed(int(time.time()))\n",
    "selected_indices = np.random.choice(len(captions), selecting_size, replace=False)\n",
    "selected_captions = [captions[i] for i in selected_indices]\n",
    "selected_urls = [urls[i] for i in selected_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@50 using L2 index: 0.17\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "# Assuming 'extractor' and 'index' are properly defined and initialized elsewhere in your code\n",
    "# along with k_list, selecting_size, selected_urls, selected_captions, train_image_urls, and testing_size\n",
    "index = l2_index  # Use the L2 index for this example\n",
    "train_image_urls = l2_train_image_urls  # Use the L2 image URLs for this example\n",
    "\n",
    "# Combine the lists and convert to a set for faster lookup\n",
    "filter_set = set(filter_out + filter_out1)\n",
    "\n",
    "# Compute the recall@k\n",
    "recall = 0\n",
    "count = 0\n",
    "for i in range(selecting_size):\n",
    "    url = selected_urls[i]\n",
    "    if url in filter_set:\n",
    "        continue  # Skip this URL if it's in the filter set\n",
    "\n",
    "    # Use regex to extract the image id\n",
    "    image_id = int(re.search(r'https?://static.flickr.com/(\\d+)/', url).group(1))\n",
    "    caption = selected_captions[i]\n",
    "\n",
    "    # Check if the image is valid\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        frame = Image.open(response.raw)\n",
    "        count += 1\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    # Get the embedding of the caption\n",
    "    query_embedding = extractor.get_text_embedding(caption)\n",
    "\n",
    "    # Initially request a larger number of neighbors\n",
    "    D, I = index.search(query_embedding, 2*k)  # Request at least 20 neighbors\n",
    "    initial_result_urls = [train_image_urls[j].strip() for j in I[0]]\n",
    "    filtered_result_ids = []\n",
    "    filtered_result_urls = []\n",
    "\n",
    "    # print('Query:', caption)\n",
    "    for result_url in initial_result_urls:\n",
    "        result_id = int(re.search(r'https?://static.flickr.com/(\\d+)/', result_url).group(1))\n",
    "        if result_id not in filter_ids:\n",
    "            filtered_result_ids.append(result_id)\n",
    "            filtered_result_urls.append(result_url)\n",
    "            if len(filtered_result_urls) >= k:\n",
    "                break\n",
    "\n",
    "    # print('Filtered Results:')\n",
    "    # for idx, result_url in enumerate(filtered_result_urls):\n",
    "    #     print('id:', filtered_result_ids[idx], 'url:', result_url)\n",
    "\n",
    "    if image_id in filtered_result_ids[:k]:  # Check only within the first k filtered results\n",
    "        recall += 1\n",
    "\n",
    "    if count >= testing_size:\n",
    "        break\n",
    "\n",
    "print(f'Recall@{k} using L2 index: {recall / testing_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@50 using HNSW index: 0.13\n"
     ]
    }
   ],
   "source": [
    "index = hnsw_index  # Use the HNSW index for this example\n",
    "train_image_urls = hnsw_train_image_urls  # Use the HNSW image URLs for this example\n",
    "\n",
    "# Compute the recall@k\n",
    "recall = 0\n",
    "count = 0\n",
    "\n",
    "for i in range(selecting_size):\n",
    "    url = selected_urls[i]\n",
    "    if url in filter_set:\n",
    "        continue  # Skip this URL if it's in the filter set\n",
    "\n",
    "    # Use regex to extract the image id\n",
    "    image_id = int(re.search(r'https?://static.flickr.com/(\\d+)/', url).group(1))\n",
    "    caption = selected_captions[i]\n",
    "\n",
    "    # Check if the image is valid\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        frame = Image.open(response.raw)\n",
    "        count += 1\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    # Get the embedding of the caption\n",
    "    query_embedding = extractor.get_text_embedding(caption)\n",
    "\n",
    "    # Initially request a larger number of neighbors\n",
    "    D, I = index.search(query_embedding, 2*k)  # Request at least 20 neighbors\n",
    "    initial_result_urls = [train_image_urls[j].strip() for j in I[0]]\n",
    "    filtered_result_ids = []\n",
    "    filtered_result_urls = []\n",
    "\n",
    "    # print('Query:', caption)\n",
    "    for result_url in initial_result_urls:\n",
    "        result_id = int(re.search(r'https?://static.flickr.com/(\\d+)/', result_url).group(1))\n",
    "        if result_id not in filter_ids:\n",
    "            filtered_result_ids.append(result_id)\n",
    "            filtered_result_urls.append(result_url)\n",
    "            if len(filtered_result_urls) >= k:\n",
    "                break\n",
    "\n",
    "    # print('Filtered Results:')\n",
    "    # for idx, result_url in enumerate(filtered_result_urls):\n",
    "    #     print('id:', filtered_result_ids[idx], 'url:', result_url)\n",
    "\n",
    "    if image_id in filtered_result_ids[:k]:  # Check only within the first k filtered results\n",
    "        recall += 1\n",
    "\n",
    "    if count >= testing_size:\n",
    "        break\n",
    "\n",
    "print(f'Recall@{k} using HNSW index: {recall / testing_size}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
