{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\softwares\\anaconda3\\envs\\transformers\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "from extract_features import mySigLipModel\n",
    "from display_image import ImageDisplay\n",
    "import time\n",
    "import numpy as np\n",
    "import requests\n",
    "from PIL import Image\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = './indexed_100k-200k/siglip_image_urls-120k-125k.json'\n",
    "index_path = './indexed_100k-200k/siglip-image-index-120k-125k.bin'\n",
    "dataset_caption_path = './dataset/SBU_captioned_photo_dataset_captions.txt'\n",
    "dataset_url_path = './dataset/SBU_captioned_photo_dataset_urls.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in the embedding extractor\n",
    "extractor = mySigLipModel()\n",
    "#load in image displayer\n",
    "display = ImageDisplay()\n",
    "#load in indexer\n",
    "index = faiss.read_index(index_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_size = 50\n",
    "k_list = [3, 5, 10, 20, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = 120000\n",
    "end_index = 125000\n",
    "with open(dataset_caption_path, 'r') as f:\n",
    "    captions = f.readlines()[start_index:end_index]\n",
    "with open(dataset_url_path, 'r') as f:\n",
    "    urls = f.readlines()[start_index:end_index]\n",
    "with open(image_path, 'r') as f:\n",
    "    train_image_urls = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select (testing_size * 1.2) images from the dataset in random\n",
    "selecting_size = int(testing_size * 1.5)  # 20% more for the case that some images are not valid\n",
    "np.random.seed(0)\n",
    "selected_indices = np.random.choice(len(captions), selecting_size, replace=False)\n",
    "selected_captions = [captions[i] for i in selected_indices]\n",
    "selected_urls = [urls[i] for i in selected_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@3: 0.14\n",
      "Recall@5: 0.22\n",
      "Recall@10: 0.34\n",
      "Recall@20: 0.46\n",
      "Recall@50: 0.5\n"
     ]
    }
   ],
   "source": [
    "# compute the recall@k\n",
    "for k in k_list:\n",
    "    recall = 0\n",
    "    count = 0\n",
    "    for i in range(selecting_size):\n",
    "        url = selected_urls[i]\n",
    "        # use regex to extract the image id in the pattern of 'http://static.flickr.com/[image_id]/XXX.jpg'\n",
    "        image_id = int(re.search(r'http://static.flickr.com/(\\d+)/', url).group(1))\n",
    "        # print('image_id: {}'.format(image_id))\n",
    "        caption = selected_captions[i]\n",
    "\n",
    "        # check if the image is valid\n",
    "        try:\n",
    "            frame = Image.open(requests.get(url, stream=True).raw)\n",
    "            count += 1\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        # get the embedding of the caption\n",
    "        query_embedding = extractor.get_text_embedding(caption)\n",
    "\n",
    "        # search the k nearest neighbors\n",
    "        D, I = index.search(query_embedding, k)\n",
    "        # check if the caption of the image is in the k nearest neighbors\n",
    "        result_urls = [train_image_urls[j].strip() for j in I[0]]\n",
    "        result_ids = [int(re.search(r'http://static.flickr.com/(\\d+)/', result_url).group(1)) for result_url in result_urls]\n",
    "        \n",
    "        # print('Query: {}'.format(caption))\n",
    "        # print('Query url: {}'.format(url))\n",
    "        # print('Results:')\n",
    "        # for j in range(k):\n",
    "        #     print('id: {}, url: {}'.format(result_ids[j], result_urls[j]))\n",
    "\n",
    "        if image_id in result_ids:\n",
    "            recall += 1\n",
    "\n",
    "        if count >= testing_size:\n",
    "            break\n",
    "\n",
    "    print('Recall@{}: {}'.format(k, recall / testing_size))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
